{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7ccc0b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f636ab6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to access COHA and get\n",
    "\n",
    "coha_dir = '/Users/gabriellachronis/Box Sync/src/lsa-predication/coha/data'\n",
    "\n",
    "output_path = './data/test_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9b29b270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target words: we want to collect tokens of each of these words from COHA\n",
    "\n",
    "target_words = ['net', 'virtual', 'disk', 'card', 'optical', 'virus',\n",
    "           'signal', 'mirror', 'energy', 'compact', 'leaf',\n",
    "           'brick', 'federal', 'sphere', 'coach', 'spine']\n",
    "\n",
    "decades = [decade for decade in np.arange(1910, 2009, 10)]\n",
    "\n",
    "buffer_size=1024\n",
    "sequence_length=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fef22b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d3b2ff89",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b99fde7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'sentence']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"this is a sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f8df86ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'an',\n",
       " 'over',\n",
       " '##long',\n",
       " 'sentence',\n",
       " 'with',\n",
       " 'contentious',\n",
       " '##ness',\n",
       " 'because',\n",
       " 'of',\n",
       " 'its',\n",
       " 'many',\n",
       " 'word',\n",
       " 'pieces']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"this is an overlong sentence with contentiousness because of its many word pieces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f65935b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['net',\n",
       " 'virtual',\n",
       " 'disk',\n",
       " 'card',\n",
       " 'optical',\n",
       " 'virus',\n",
       " 'signal',\n",
       " 'mirror',\n",
       " 'energy',\n",
       " 'compact',\n",
       " 'leaf',\n",
       " 'brick',\n",
       " 'federal',\n",
       " 'sphere',\n",
       " 'coach',\n",
       " 'spine']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure our targets aren't made of more than one word piece\n",
    "tokenizer.tokenize(' '.join(target_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "59d7d1ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 5658,\n",
       " 7484,\n",
       " 9785,\n",
       " 4003,\n",
       " 9380,\n",
       " 7865,\n",
       " 4742,\n",
       " 5259,\n",
       " 2943,\n",
       " 9233,\n",
       " 7053,\n",
       " 5318,\n",
       " 2976,\n",
       " 10336,\n",
       " 2873,\n",
       " 8560,\n",
       " 102]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# when we encode the target words they'll get the cls and sep tokens added\n",
    "tokenizer.encode(' '.join(target_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d1dd0953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5658,\n",
       " 7484,\n",
       " 9785,\n",
       " 4003,\n",
       " 9380,\n",
       " 7865,\n",
       " 4742,\n",
       " 5259,\n",
       " 2943,\n",
       " 9233,\n",
       " 7053,\n",
       " 5318,\n",
       " 2976,\n",
       " 10336,\n",
       " 2873,\n",
       " 8560]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so we have to take it away\n",
    "tokenizer.encode(' '.join(target_words))[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8cc251a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(token_ids, target_position, sequence_length=128):\n",
    "    \"\"\"\n",
    "    Given a text containing a target word, return the sentence snippet which surrounds the target word\n",
    "    (and the target word's position in the snippet).\n",
    "\n",
    "    :param token_ids: list of token ids (for an entire line of text)\n",
    "    :param target_position: index of the target word's position in `tokens`\n",
    "    :param sequence_length: desired length for output sequence (e.g. 128, 256, 512)\n",
    "    :return: (context_ids, new_target_position)\n",
    "                context_ids: list of token ids for the output sequence\n",
    "                new_target_position: index of the target word's position in `context_ids`\n",
    "    \"\"\"\n",
    "    # -2 as [CLS] and [SEP] tokens will be added later; /2 as it's a one-sided window\n",
    "    window_size = int((sequence_length - 2) / 2)\n",
    "    context_start = max([0, target_position - window_size])\n",
    "    padding_offset = max([0, window_size - target_position])\n",
    "    padding_offset += max([0, target_position + window_size - len(token_ids)])\n",
    "\n",
    "    context_ids = token_ids[context_start:target_position + window_size]\n",
    "    #print(token_ids[target_position])\n",
    "    #print(context_ids)\n",
    "    #print(tokenizer.convert_ids_to_tokens(context_ids))\n",
    "    #print('next')\n",
    "    \n",
    "    context_ids += padding_offset * [0]\n",
    "\n",
    "    new_target_position = target_position - context_start\n",
    "\n",
    "    return context_ids, new_target_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c552faf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{5658: 'net', 7484: 'virtual', 9785: 'disk', 4003: 'card', 9380: 'optical', 7865: 'virus', 4742: 'signal', 5259: 'mirror', 2943: 'energy', 9233: 'compact', 7053: 'leaf', 5318: 'brick', 2976: 'federal', 10336: 'sphere', 2873: 'coach', 8560: 'spine'}\n"
     ]
    }
   ],
   "source": [
    "# build word-index vocabulary for target words\n",
    "i2w = {}\n",
    "for t, t_id in zip(target_words, tokenizer.encode(' '.join(target_words))[1:-1]): # use [1:-1] to not include cls and sep\n",
    "    i2w[t_id] = t\n",
    "    \n",
    "print(i2w)\n",
    "\n",
    "# buffers for batch processing\n",
    "batch_input_ids = []\n",
    "batch_tokens = []\n",
    "batch_pos = []\n",
    "batch_snippets = []\n",
    "batch_decades = []\n",
    "\n",
    "# here is where we'll store our final list we are collecting\n",
    "usages = defaultdict(list)  # w -> (vector, sentence, word_position, decade)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103e9cba",
   "metadata": {},
   "source": [
    "we need these lists to be the same length before we zip them! We accidentally had our zipper on wrong like when you button a shirt and theres a leftover buttton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7f91474e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(' '.join(target_words))[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "87ed81df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cb2ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decade 1910...\n",
      "/Users/gabriellachronis/Box Sync/src/lsa-predication/coha/data\n",
      "1910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3355/3355 [13:22:27<00:00, 14.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decade 1920...\n",
      "/Users/gabriellachronis/Box Sync/src/lsa-predication/coha/data\n",
      "1920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11557/11557 [3:01:22<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decade 1930...\n",
      "/Users/gabriellachronis/Box Sync/src/lsa-predication/coha/data\n",
      "1930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██████████▊                                                                                                                                                          | 677/10352 [04:13<1:20:34,  2.00it/s]"
     ]
    }
   ],
   "source": [
    "for T, decade in enumerate(decades):\n",
    "    # one time interval at a time\n",
    "    print('Decade {}...'.format(decade))\n",
    "\n",
    "\n",
    "    ### gabriella changes\n",
    "    ### my coha is organized differently. \n",
    "    ### the decades have random numbers for the alphabet index places , so i have to use regex\n",
    "    ### to ignore that. \n",
    "    print(coha_dir)\n",
    "    print(decade)\n",
    "    my_regex = r'text_' + re.escape(str(decade)) + 's.*'\n",
    "\n",
    "    #print(\"running through decade \", decade)\n",
    "\n",
    "    # iterate through directories\n",
    "    for decade_dir in os.listdir(coha_dir):\n",
    "\n",
    "        if re.match(my_regex, decade_dir):\n",
    "            # get all the text files for that decade\n",
    "            # iterate through text files for this decade\n",
    "            this_decade_files = os.listdir(os.path.join(coha_dir, decade_dir))\n",
    "            for filename in tqdm(this_decade_files):\n",
    "                #print(filename)\n",
    "                with open(os.path.join(coha_dir, decade_dir, filename), 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                    #print(\"gets here\")\n",
    "\n",
    "                    # get the usages from this file\n",
    "                    for L, line in enumerate(lines):\n",
    "                        #print(\"gets to line: \", L)\n",
    "                        \n",
    "                        #print(len(tokens))\n",
    "                        for token in tokens:\n",
    "\n",
    "                            # tokenize line and convert to token ids\n",
    "                            tokens = tokenizer.encode(line)\n",
    "\n",
    "                            for pos, token in enumerate(tokens):\n",
    "                                #print(token)\n",
    "                                # store usage info of target words only\n",
    "                                if token in i2w:\n",
    "                                    context_ids, pos_in_context = get_context(tokens, pos, sequence_length)\n",
    "\n",
    "                                    input_ids = [101] + context_ids + [102]\n",
    "\n",
    "\n",
    "                                    # convert later to save storage space\n",
    "                                    snippet = tokenizer.convert_ids_to_tokens(context_ids)\n",
    "                                    #print(i2w[token])\n",
    "                                    #print(' '.join(snippet))\n",
    "\n",
    "                                    # add usage info to buffers\n",
    "                                    batch_input_ids.append(input_ids)\n",
    "                                    batch_tokens.append(i2w[token])\n",
    "                                    batch_pos.append(pos_in_context)\n",
    "                                    batch_snippets.append(snippet)\n",
    "                                    batch_decades.append(decade)\n",
    "\n",
    "                                # if the buffers are full...             or if we're at the end of the dataset\n",
    "                                if (len(batch_input_ids) >= buffer_size) or (L == len(lines) - 1 and T == len(decades) - 1):\n",
    "\n",
    "#                                     with torch.no_grad():\n",
    "#                                         # collect list of input ids into a single batch tensor\n",
    "#                                         input_ids_tensor = torch.tensor(batch_input_ids)\n",
    "#                                         if torch.cuda.is_available():\n",
    "#                                             input_ids_tensor = input_ids_tensor.to('cuda')\n",
    "\n",
    "#                                         # run usages through language model\n",
    "#                                         outputs = model(input_ids_tensor,  output_hidden_states=True )\n",
    "#                                         print(len(outputs.hidden_states)) # items in the tuple = 1 + num layers\n",
    "#                                         if torch.cuda.is_available():\n",
    "#                                             hidden_states = [l.detach().cpu().clone().numpy() for l in outputs[2]]\n",
    "#                                         else:\n",
    "#                                             print(\"fjekl\")\n",
    "#                                             hidden_states = [l.clone().numpy() for l in outputs.hidden_states]\n",
    "\n",
    "#                                         # get usage vectors from hidden states\n",
    "#                                         hidden_states = np.stack(hidden_states)  # (13, B, |s|, 768)\n",
    "#                                         print('Expected hidden states size: (13, B, |s|, 768). Got {}'.format(hidden_states.shape))\n",
    "#                                         # usage_vectors = np.sum(hidden_states, 0)  # (B, |s|, 768)\n",
    "#                                         # usage_vectors = hidden_states.view(hidden_states.shape[1],\n",
    "#                                         #                                    hidden_states.shape[2],\n",
    "#                                         #                                    -1)\n",
    "#                                         usage_vectors = np.sum(hidden_states[1:, :, :, :], axis=0)\n",
    "#                                         # usage_vectors = hidden_states.reshape((hidden_states.shape[1], hidden_states.shape[2], -1))\n",
    "#                                         print(\"makes usage vectors\")\n",
    "#                                         print(usage_vectors.shape)\n",
    "\n",
    "                                    if output_path and os.path.exists(output_path):\n",
    "                                        with open(output_path, 'rb') as f:\n",
    "                                            usages = pickle.load(f)\n",
    "\n",
    "                                    # store usage tuples in a dictionary: lemma -> (vector, snippet, position, decade)\n",
    "                                    for b in np.arange(len(batch_input_ids)):\n",
    "                                        #usage_vector = usage_vectors[b, batch_pos[b]+1, :] # get the right position\n",
    "                                        usages[batch_tokens[b]].append(\n",
    "                                            (batch_snippets[b], batch_pos[b], batch_decades[b]))\n",
    "\n",
    "                                    # finally, empty the batch buffers\n",
    "                                    batch_input_ids, batch_tokens, batch_pos, batch_snippets, batch_decades = [], [], [], [], []\n",
    "\n",
    "                                    # and store data incrementally\n",
    "                                    if output_path:\n",
    "                                        with open(output_path, 'wb') as f:\n",
    "                                            pickle.dump(usages, file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c5b38a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbb348d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b7f7a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
