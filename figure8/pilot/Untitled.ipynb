{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from skbio.stats.distance import mantel\n",
    "from scipy.stats import entropy\n",
    "from operator import itemgetter\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "with open('../aggregate-avg-all.csv', newline='\\n', mode='r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    for row in reader:\n",
    "        all_data.append(row)\n",
    "\n",
    "f2i = {}\n",
    "for i, f in enumerate(all_data[0]):\n",
    "    f2i[f] = i\n",
    "i2f = {i: f for (f, i) in f2i.items()}\n",
    "\n",
    "# remove fields row\n",
    "all_data = all_data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_unit_id': 0,\n",
       " '_golden': 1,\n",
       " '_unit_state': 2,\n",
       " '_trusted_judgments': 3,\n",
       " '_last_judgment_at': 4,\n",
       " 'cannot_decide_reason': 5,\n",
       " 'sim_score': 6,\n",
       " 'sim_score:variance': 7,\n",
       " 'a': 8,\n",
       " 'b': 9,\n",
       " 'cannot_decide_reason_gold': 10,\n",
       " 'cluster_a': 11,\n",
       " 'cluster_b': 12,\n",
       " 'id_a': 13,\n",
       " 'id_b': 14,\n",
       " 'lemma': 15,\n",
       " 'sim_score_gold': 16,\n",
       " 'time_a': 17,\n",
       " 'time_b': 18}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2540617972',\n",
       " 'false',\n",
       " 'finalized',\n",
       " '5',\n",
       " '11/13/2019 17:50:08',\n",
       " '',\n",
       " '2.2',\n",
       " '0.98',\n",
       " 'undead be born . [[virus]] can not be traced to rotten meat , obscene sexual rite of autumnal haystack , nor the french itch . maybe the devil have made this devil thing .',\n",
       " \"crack at the tests - - different ones , of course . he was under the weather when he took them the first time - - tail end of some [[virus]] business . he did n ' t miss by much , and the appeal board made a special ruling .\",\n",
       " '',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '3',\n",
       " 'virus',\n",
       " '',\n",
       " '1970-80',\n",
       " '1950-60']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "snippets = defaultdict(dict)\n",
    "judgements = defaultdict(dict)\n",
    "variances = defaultdict(dict)\n",
    "\n",
    "for datum in all_data:\n",
    "    lemma = datum[f2i['lemma']]\n",
    "    id_a = int(datum[f2i['id_a']])\n",
    "    id_b = int(datum[f2i['id_b']])\n",
    "    a = datum[f2i['a']]\n",
    "    b = datum[f2i['b']]\n",
    "    \n",
    "    if a not in snippets[lemma]:\n",
    "        snippets[lemma][id_a] = a.lower()\n",
    "    \n",
    "    if b not in snippets[lemma]:\n",
    "        snippets[lemma][id_b] = b.lower()\n",
    "        \n",
    "    judgements[lemma][(id_a, id_b)] = float(datum[f2i['sim_score']])\n",
    "    variances[lemma][(id_a, id_b)] = float(datum[f2i['sim_score:variance']])\n",
    "    \n",
    "    \n",
    "for w in snippets:\n",
    "    for id_, sent in snippets[w].items():\n",
    "        tokens = list(map(str.lower, sent.split()))\n",
    "        \n",
    "        form = None\n",
    "        for t in tokens:\n",
    "            if t.startswith('[[') and t.endswith(']]'):\n",
    "                form = t[2:-2]\n",
    "\n",
    "        snippets[w][id_] = (form, sent)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for w in variances:\n",
    "    print(w, np.mean(list(variances[w].values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrices = {}\n",
    "\n",
    "for w in judgements:\n",
    "    n_sent = len(snippets[w])\n",
    "    m = np.zeros((n_sent, n_sent))\n",
    "\n",
    "    for (id_a, id_b), score in judgements[w].items():\n",
    "        m[id_a, id_b] = float(score)\n",
    "        m[id_b, id_a] = float(score)\n",
    "\n",
    "    sim_matrices[w] = m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 gye and barnard wanted to make no such mistake . furthermore , they knew that with the isolation of the [[virus]] the battle would be only half won . a cure must be found later .\n",
      "1 undead be born . [[virus]] can not be traced to rotten meat , obscene sexual rite of autumnal haystack , nor the french itch . maybe the devil have made this devil thing .\n",
      "2 as the others entered she opened one eye in blissful content . \" i ' m afraid , \" she announced , \" that i ' m getting the village [[virus]] . i do n ' t want to move , i do n ' t want to stir , i do n ' t even want to lift a finger - - ever .\n",
      "3 crack at the tests - - different ones , of course . he was under the weather when he took them the first time - - tail end of some [[virus]] business . he did n ' t miss by much , and the appeal board made a special ruling .\n",
      "4 solomon ' s software , recently acquired by data - fellows , offers a pair of virus scanners : dr . solomon ' s anti - [[virus]] deluxe for windows 95 / nt and dr . solomon ' s virex for the macintosh os .\n",
      "5  recent theories . [[virus]] . virus , of many forms , appears in every cancer patient and vitiates his blood , upsets the biochemistry homologous to the normal for the species , so that the organism can not repair the damage done to the locale of infection or irritation .\n",
      "6 80 % of the 28 , 976 but it increasingly appears that the number of reported aids cases is far smaller than the true incidence of the disease . halfdan mahler , director general of who , estimates that there may be as many as 100 , 000 cases globally , plus up to 10 million carriers of the [[virus]] . the disease is spreading fast in africa , where it was first detected in 1982 , but is believed by some medical researchers to have appeared decades earlier .\n",
      "7  south all the way to the isthmus of tehuantepec ( where another , shorter fence will be built ) , veterinarians are experimenting with aftosa vaccines shipped from the netherlands and argentina . if they work against the mexican [[virus]] , and if the government can persuade skeptical campesinos of the necessity of sticking a needle into their animals every six months , mexico hopes at least to control aftosa . u .\n",
      "8 public health service experts . advocates of live - [[virus]] vaccines maintain that their preparations , taken in one or three doses , confer immunity in 90 % or more of the vaccinated , as measured by laboratory tests of blood antibodies against polio viruses . but do the oral vaccines really give such a high proportional protection against paralysis ?\n",
      "9 ' the news was good . his level of t - 4 cells - - the immune cells killed by the aids [[virus]] - - was still relatively high . and he had no measurable trace of the virus in his blood .\n",
      "[[0.  2.  2.  ... 2.2 2.  3. ]\n",
      " [2.  0.  2.6 ... 2.  1.8 2.4]\n",
      " [2.  2.6 0.  ... 2.2 2.  2.6]\n",
      " ...\n",
      " [2.2 2.  2.2 ... 0.  1.4 2.8]\n",
      " [2.  1.8 2.  ... 1.4 0.  1.8]\n",
      " [3.  2.4 2.6 ... 2.8 1.8 0. ]]\n"
     ]
    }
   ],
   "source": [
    "for i, s in sorted(snippets['virus'].items(), key=itemgetter(0)):\n",
    "    print(i, s[1])\n",
    "print(sim_matrices['leaf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "\n",
    "# lm = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "lm = BertModel.from_pretrained(\n",
    "    'bert-large-uncased',\n",
    "    output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "\n",
    "def canberra_distance(a, b):\n",
    "    return np.sum(np.abs(a-b) / (np.abs(a) + np.abs(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_sim_matrices = {}\n",
    "for lemma in tqdm(judgements):\n",
    "# for lemma in ['virus', 'sphere', 'leaf', 'card']:\n",
    "#     print('--- Lemma: {}'.format(lemma))\n",
    "\n",
    "    bert_sim_matrices[lemma] = np.zeros_like(sim_matrices[lemma])\n",
    "\n",
    "    for (id_a, id_b) in judgements[w]:\n",
    "        \n",
    "        form1, s1 = snippets[lemma][id_a]\n",
    "        form2, s2 = snippets[lemma][id_b]\n",
    "\n",
    "        tokens_s1 = tokenizer.tokenize(s1)\n",
    "        tokens_s2 = tokenizer.tokenize(s2)\n",
    "\n",
    "        new_tokens_s1 = []\n",
    "        skip_till = -1\n",
    "        target1_pos = None\n",
    "        for i, tok in enumerate(tokens_s1):\n",
    "            if i <= skip_till:\n",
    "                continue\n",
    "            if tok == '[' and tokens_s1[i+1] == '[' and tokens_s1[i+2] == form1:\n",
    "                skip_till = i+4\n",
    "                target1_pos = len(new_tokens_s1)\n",
    "                new_tokens_s1.append(form1)\n",
    "            elif tok == '[' and tokens_s1[i + 1] == '[' and tokens_s1[i + 2] == lemma and tokens_s1[i + 3].startswith('##'):\n",
    "                skip_till = i + 5\n",
    "                target1_pos = len(new_tokens_s1)\n",
    "                new_tokens_s1.append(lemma)\n",
    "                new_tokens_s1.append(tokens_s1[i + 3])\n",
    "            else:\n",
    "                new_tokens_s1.append(tok)\n",
    "\n",
    "        new_tokens_s2 = []\n",
    "        skip_till = -1\n",
    "        target2_pos = None\n",
    "        for i, tok in enumerate(tokens_s2):\n",
    "            if i <= skip_till:\n",
    "                continue\n",
    "            if tok == '[' and tokens_s2[i + 1] == '[' and tokens_s2[i + 2] == form2:\n",
    "                skip_till = i + 4\n",
    "                target2_pos = len(new_tokens_s2)\n",
    "                new_tokens_s2.append(form2)\n",
    "            elif tok == '[' and tokens_s2[i + 1] == '[' and tokens_s2[i + 2] == lemma and tokens_s2[i + 3].startswith('##'):\n",
    "                skip_till = i + 5\n",
    "                target2_pos = len(new_tokens_s2)\n",
    "                new_tokens_s2.append(lemma)\n",
    "                new_tokens_s2.append(tokens_s2[i + 3])\n",
    "            else:\n",
    "                new_tokens_s2.append(tok)\n",
    "\n",
    "\n",
    "        token_ids_1 = tokenizer.encode(new_tokens_s1)\n",
    "        token_ids_2 = tokenizer.encode(new_tokens_s2)\n",
    "        \n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            input_ids_tensor_1 = torch.tensor([token_ids_1])\n",
    "            input_ids_tensor_2 = torch.tensor([token_ids_2])\n",
    "\n",
    "            outputs_1 = lm(input_ids_tensor_1)\n",
    "            outputs_2 = lm(input_ids_tensor_2)\n",
    "            \n",
    "#             print(outputs_1[0].shape, outputs_2[1].shape)\n",
    "\n",
    "            hidden_states_1 = np.stack([l.clone().numpy() for l in outputs_1[2]])\n",
    "            hidden_states_2 = np.stack([l.clone().numpy() for l in outputs_2[2]])\n",
    "\n",
    "            hidden_states_1 = hidden_states_1.squeeze(1)\n",
    "            hidden_states_2 = hidden_states_2.squeeze(1)\n",
    "\n",
    "            usage_vector_1 = hidden_states_1[:, target1_pos, :]\n",
    "            usage_vector_2 = hidden_states_2[:, target2_pos, :]\n",
    "\n",
    "#             usage_vector_1 = np.sum(usage_vector_1, axis=0)\n",
    "#             usage_vector_2 = np.sum(usage_vector_2, axis=0)\n",
    "            usage_vector_1 = usage_vector_1.reshape((usage_vector_1.shape[0] * usage_vector_1.shape[1]))\n",
    "            usage_vector_2 = usage_vector_2.reshape((usage_vector_2.shape[0] * usage_vector_2.shape[1]))\n",
    "# \n",
    "#             print(usage_vector_1.shape, usage_vector_2.shape)\n",
    "\n",
    "            sim_score = cosine_similarity(usage_vector_1, usage_vector_2)\n",
    "            bert_sim_matrices[lemma][id_a, id_b] = sim_score\n",
    "            bert_sim_matrices[lemma][id_b, id_a] = sim_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = {}\n",
    "sig_coeffs = {}\n",
    "\n",
    "for w in bert_sim_matrices:\n",
    "    coeff, p_value, n = mantel(\n",
    "        sim_matrices[w],\n",
    "        bert_sim_matrices[w],\n",
    "        method='spearman',  # pearson\n",
    "        permutations=999,\n",
    "        alternative='two-sided'  # greater, less\n",
    "    )\n",
    "\n",
    "    print(w)\n",
    "    print('spearman: {:.2f}    p: {:.2f}'.format(coeff, p_value))\n",
    "\n",
    "    coeffs[w] = coeff\n",
    "\n",
    "    if p_value < 0.05:\n",
    "        sig_coeffs[w] = coeff\n",
    "        print('---')\n",
    "\n",
    "print(np.mean(list(coeffs.values())))\n",
    "print(np.mean(list(sig_coeffs.values())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_entropy(scores):\n",
    "    p_distr = np.zeros(5)\n",
    "    for c in range(5):\n",
    "        p_distr[c] = scores.count(c) / len(scores)\n",
    "    \n",
    "    return entropy(p_distr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3285 rows.\n",
      "\n",
      "Fields:\n",
      "['_unit_id', '_golden', '_unit_state', '_trusted_judgments', '_last_judgment_at', 'cannot_decide_reason', 'sim_score', 'a', 'b', 'cannot_decide_reason_gold', 'cluster_a', 'cluster_b', 'id_a', 'id_b', 'lemma', 'sim_score_gold', 'time_a', 'time_b']\n"
     ]
    }
   ],
   "source": [
    "agg_data = []\n",
    "with open('../aggregate-all-all.csv', newline='\\n', mode='r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    for row in reader:\n",
    "        agg_data.append(row)\n",
    "\n",
    "field2index = {}\n",
    "for i, f in enumerate(agg_data[0]):\n",
    "    field2index[f] = i\n",
    "index2field = {i: f for (f, i) in field2index.items()}\n",
    "\n",
    "# remove fields row\n",
    "agg_data = agg_data[1:]\n",
    "\n",
    "print('{} rows.\\n'.format(len(agg_data)))\n",
    "print('Fields:\\n{}'.format(list(index2field.values())))\n",
    "\n",
    "variances_bylemma = defaultdict(list)\n",
    "entropies_bylemma = defaultdict(list)\n",
    "for row in agg_data:\n",
    "    lemma = row[field2index['lemma']]\n",
    "    row_judgements = list(map(int, row[field2index['sim_score']].split('\\n')))\n",
    "    variances_bylemma[lemma].append(np.var(row_judgements))\n",
    "    entropies_bylemma[lemma].append(ann_entropy(row_judgements))\n",
    "        \n",
    "        \n",
    "mean_variance_bylemma = {}\n",
    "median_variance_bylemma = {}\n",
    "max_variance_bylemma = {}\n",
    "min_variance_bylemma = {}\n",
    "median_entropy_bylemma = {}\n",
    "mean_entropy_bylemma = {}\n",
    "min_entropy_bylemma = {}\n",
    "\n",
    "for lemma, variances in variances_bylemma.items():\n",
    "    mean_variance_bylemma[lemma] = np.mean(variances)\n",
    "    median_variance_bylemma[lemma] = np.median(variances)\n",
    "    max_variance_bylemma[lemma] = np.max(variances)\n",
    "    \n",
    "    v = np.ma.masked_equal(variances, 0.0, copy=False)\n",
    "    min_variance_bylemma[lemma] = np.min(v)\n",
    "    \n",
    "    median_entropy_bylemma[lemma] = np.median(entropies_bylemma[lemma])\n",
    "    mean_entropy_bylemma[lemma] = np.mean(entropies_bylemma[lemma])\n",
    "    \n",
    "    h = np.ma.masked_equal(entropies_bylemma[lemma], np.inf, copy=False)\n",
    "    min_entropy_bylemma[lemma] = np.min(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "print('Median ', spearmanr(list(coeffs.values()), list(median_variance_bylemma.values())))\n",
    "print('Mean   ', spearmanr(list(coeffs.values()), list(mean_variance_bylemma.values())))\n",
    "print('Max    ', spearmanr(list(coeffs.values()), list(max_variance_bylemma.values())))\n",
    "\n",
    "print('Median ', spearmanr(list(coeffs.values()), list(median_entropy_bylemma.values())))\n",
    "print('Mean   ', spearmanr(list(coeffs.values()), list(mean_entropy_bylemma.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, Y = zip(*sorted(variance_bylemma.items(), key=itemgetter(1)))\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='Agreement', x=list(coeffs.keys()), y=list(coeffs.values())),\n",
    "    go.Bar(name='Variance', x=list(variance_bylemma.keys()), y=list(variance_bylemma.values()))\n",
    "])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('median_var.txt', 'w') as f:\n",
    "    for w, var in sorted(median_variance_bylemma.items(), key=itemgetter(1)):\n",
    "        print('{:10} {:.3f}'.format(w, var), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w, var in sorted(mean_entropy_bylemma.items(), key=itemgetter(1)):\n",
    "    print('{:10} {:.2f}'.format(w, var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_bycoder = sorted(annotations_bylemma['virus'], key=itemgetter(0)) # 0:coder 1:item 2:label \n",
    "A_byitem = sorted(annotations_bylemma['virus'], key=itemgetter(1, 0)) # 0:coder 1:item 2:label \n",
    "\n",
    "\n",
    "coder2idx = {}\n",
    "for (coder, _, _) in A_bycoder:\n",
    "    if coder not in coder2idx:\n",
    "        coder2idx[coder] = len(coder2idx)\n",
    "\n",
    "item2idx = {}\n",
    "for (coder, item, _) in A_byitem:\n",
    "    if item not in item2idx:\n",
    "        item2idx[item] = len(item2idx)\n",
    "\n",
    "agr_data = [['*' for _ in item2idx] for _ in coder2idx]\n",
    "\n",
    "for (coder, item, label) in A_byitem: \n",
    "    agr_data[coder2idx[coder]][item2idx[item]] = str(label)\n",
    "\n",
    "agr_data = tuple(tuple(l) for l in agr_data)\n",
    "\n",
    "missing = '*' # indicator for missing values\n",
    "print(\"interval metric: %.4f\" % krippendorff_alpha(agr_data, interval_metric, missing_items=missing))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
